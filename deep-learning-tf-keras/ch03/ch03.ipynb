{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[ 0.22355038]\n",
      " [ 0.91425949]\n",
      " [ 0.91425949]\n",
      " [ 0.99747425]]\n",
      "w: [[ 3.61188436]\n",
      " [ 3.61188436]]\n",
      "b: [-1.24509501]\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow logistic regression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(0)  # 乱数シード\n",
    "\n",
    "# weight, bias\n",
    "w = tf.Variable(tf.zeros([2, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# model y = sigmmoid(w^T*x + b)\n",
    "# input x\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "# teacher\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "# output y\n",
    "y = tf.nn.sigmoid(tf.matmul(x, w) + b) \n",
    "\n",
    "cross_entropy = - tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "# 確認\n",
    "correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "\n",
    "# OR logic\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(200):\n",
    "    sess.run(train_step, feed_dict={\n",
    "            x: X,\n",
    "            t: Y\n",
    "        })\n",
    "\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })\n",
    "print(classified)\n",
    "\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })\n",
    "\n",
    "print(prob)\n",
    "\n",
    "# 学習したパラメータの確認\n",
    "print('w:', sess.run(w))\n",
    "print('b:', sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s - loss: 0.9976     \n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s - loss: 0.8754     \n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s - loss: 0.7831     \n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s - loss: 0.7140     \n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s - loss: 0.6599     \n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s - loss: 0.6187     \n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s - loss: 0.5869     \n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s - loss: 0.5613     \n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s - loss: 0.5399     \n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s - loss: 0.5227     \n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s - loss: 0.5084     \n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s - loss: 0.4954     \n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s - loss: 0.4839     \n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s - loss: 0.4738     \n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s - loss: 0.4651     \n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s - loss: 0.4566     \n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s - loss: 0.4488     \n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s - loss: 0.4416     \n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s - loss: 0.4346     \n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s - loss: 0.4282     \n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s - loss: 0.4222     \n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s - loss: 0.4162     \n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s - loss: 0.4105     \n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s - loss: 0.4050     \n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s - loss: 0.3997     \n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s - loss: 0.3945     \n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s - loss: 0.3895     \n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s - loss: 0.3846     \n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s - loss: 0.3800     \n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s - loss: 0.3754     \n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s - loss: 0.3709     \n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s - loss: 0.3665     \n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s - loss: 0.3621     \n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s - loss: 0.3579     \n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s - loss: 0.3538     \n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s - loss: 0.3498     \n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s - loss: 0.3458     \n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s - loss: 0.3419     \n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s - loss: 0.3381     \n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s - loss: 0.3344     \n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s - loss: 0.3308     \n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s - loss: 0.3272     \n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s - loss: 0.3237     \n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s - loss: 0.3202     \n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s - loss: 0.3169     \n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s - loss: 0.3135     \n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s - loss: 0.3103     \n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s - loss: 0.3071     \n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s - loss: 0.3039     \n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s - loss: 0.3008     \n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s - loss: 0.2978     \n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s - loss: 0.2949     \n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s - loss: 0.2919     \n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s - loss: 0.2891     \n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s - loss: 0.2862     \n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s - loss: 0.2834     \n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s - loss: 0.2808     \n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s - loss: 0.2781     \n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s - loss: 0.2755     \n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s - loss: 0.2728     \n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s - loss: 0.2703     \n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s - loss: 0.2679     \n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s - loss: 0.2654     \n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s - loss: 0.2630     \n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s - loss: 0.2606     \n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s - loss: 0.2583     \n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s - loss: 0.2560     \n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s - loss: 0.2537     \n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s - loss: 0.2515     \n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s - loss: 0.2493     \n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s - loss: 0.2472     \n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s - loss: 0.2451     \n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s - loss: 0.2430     \n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s - loss: 0.2409     \n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s - loss: 0.2389     \n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s - loss: 0.2369     \n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s - loss: 0.2350     \n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s - loss: 0.2330     \n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s - loss: 0.2312     \n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s - loss: 0.2293     \n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s - loss: 0.2275     \n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s - loss: 0.2256     \n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s - loss: 0.2239     \n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s - loss: 0.2221     \n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s - loss: 0.2204     \n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s - loss: 0.2186     \n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s - loss: 0.2170     \n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s - loss: 0.2153     \n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s - loss: 0.2137     \n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s - loss: 0.2121     \n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s - loss: 0.2105     \n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s - loss: 0.2089     \n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s - loss: 0.2074     \n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s - loss: 0.2058     \n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s - loss: 0.2043     \n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s - loss: 0.2028     \n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s - loss: 0.2014     \n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s - loss: 0.1999     \n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s - loss: 0.1985     \n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s - loss: 0.1971     \n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s - loss: 0.1957     \n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s - loss: 0.1943     \n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s - loss: 0.1930     \n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s - loss: 0.1917     \n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s - loss: 0.1904     \n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s - loss: 0.1890     \n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s - loss: 0.1878     \n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s - loss: 0.1853     \n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s - loss: 0.1840     \n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s - loss: 0.1828     \n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s - loss: 0.1816     \n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s - loss: 0.1804     \n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s - loss: 0.1792     \n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s - loss: 0.1781     \n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s - loss: 0.1769     \n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s - loss: 0.1758     \n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s - loss: 0.1747     \n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s - loss: 0.1736     \n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s - loss: 0.1725     \n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s - loss: 0.1714     \n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s - loss: 0.1703     \n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s - loss: 0.1693     \n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s - loss: 0.1682     \n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s - loss: 0.1672     \n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s - loss: 0.1662     \n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s - loss: 0.1652     \n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s - loss: 0.1642     \n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s - loss: 0.1632     \n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s - loss: 0.1622     \n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s - loss: 0.1612     \n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s - loss: 0.1603     \n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s - loss: 0.1594     \n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s - loss: 0.1584     \n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s - loss: 0.1575     \n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s - loss: 0.1566     \n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s - loss: 0.1557     \n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s - loss: 0.1548     \n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s - loss: 0.1539     \n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s - loss: 0.1530     \n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s - loss: 0.1522     \n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s - loss: 0.1513     \n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s - loss: 0.1505     \n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s - loss: 0.1496     \n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s - loss: 0.1488     \n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s - loss: 0.1480     \n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s - loss: 0.1472     \n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s - loss: 0.1464     \n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s - loss: 0.1456     \n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s - loss: 0.1448     \n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s - loss: 0.1440     \n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s - loss: 0.1433     \n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s - loss: 0.1425     \n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s - loss: 0.1418     \n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s - loss: 0.1410     \n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s - loss: 0.1403     \n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s - loss: 0.1395     \n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s - loss: 0.1388     \n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s - loss: 0.1381     \n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s - loss: 0.1374     \n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s - loss: 0.1367     \n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s - loss: 0.1360     \n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s - loss: 0.1353     \n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s - loss: 0.1346     \n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s - loss: 0.1339     \n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s - loss: 0.1332     \n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s - loss: 0.1313     \n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s - loss: 0.1306     \n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s - loss: 0.1287     \n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s - loss: 0.1251     \n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s - loss: 0.1245     \n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s - loss: 0.1239     \n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s - loss: 0.1233     \n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s - loss: 0.1227     \n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s - loss: 0.1222     \n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s - loss: 0.1216     \n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s - loss: 0.1210     \n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s - loss: 0.1205     \n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s - loss: 0.1199     \n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s - loss: 0.1194     \n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s - loss: 0.1188     \n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s - loss: 0.1183     \n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s - loss: 0.1178     \n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s - loss: 0.1172     \n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s - loss: 0.1167     \n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s - loss: 0.1162     \n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s - loss: 0.1157     \n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s - loss: 0.1152     \n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s - loss: 0.1147     \n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s - loss: 0.1142     \n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s - loss: 0.1137     \n",
      "1/4 [======>.......................] - ETA: 0sclassified\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "output probability:\n",
      "[[ 0.23046966]\n",
      " [ 0.91723806]\n",
      " [ 0.90638953]\n",
      " [ 0.99721682]]\n"
     ]
    }
   ],
   "source": [
    "# Keras logistic regression\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_dim=2, units=1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[1]])\n",
    "model.fit(X,Y,epochs=200,batch_size=1)\n",
    "classes = model.predict_classes(X, batch_size=1)\n",
    "prob = model.predict_proba(X, batch_size=1)\n",
    "\n",
    "print('classified')\n",
    "print(Y == classes)\n",
    "print()\n",
    "\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  2.51478720e-02   9.72906291e-01   1.94587547e-03]\n",
      " [  1.48497477e-01   8.44408453e-01   7.09398324e-03]\n",
      " [  9.87741709e-01   1.22583313e-02   2.51842600e-08]\n",
      " [  9.71690059e-01   2.83099897e-02   6.18570262e-09]\n",
      " [  9.90506709e-01   9.49331466e-03   3.73369824e-09]\n",
      " [  9.57944930e-01   4.20549363e-02   6.18812166e-08]\n",
      " [  2.07735859e-02   9.76254106e-01   2.97234813e-03]\n",
      " [  9.95067120e-01   4.93289670e-03   4.34059633e-09]\n",
      " [  2.32827384e-02   9.73150969e-01   3.56635731e-03]\n",
      " [  9.05207917e-03   9.84450161e-01   6.49771141e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Multi Logistic Regression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "M = 2 # dimension of input\n",
    "K = 3 # number of class\n",
    "n = 100 # number of data per class\n",
    "N = n * K # all data\n",
    "\n",
    "# sample data\n",
    "X1 = np.random.randn(n, M) + np.array([0, 10])\n",
    "X2 = np.random.randn(n, M) + np.array([5, 5])\n",
    "X3 = np.random.randn(n, M) + np.array([10, 0])\n",
    "Y1 = np.array([[1,0,0] for _ in range(n)])\n",
    "Y2 = np.array([[0,1,0] for _ in range(n)])\n",
    "Y3 = np.array([[0,0,1] for _ in range(n)])\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis=0)\n",
    "Y = np.concatenate((Y1, Y2, Y3), axis=0)\n",
    "\n",
    "tf.set_random_seed(0)  # 乱数シード\n",
    "\n",
    "# weight, bias\n",
    "w = tf.Variable(tf.zeros([M, K]))\n",
    "b = tf.Variable(tf.zeros([K]))\n",
    "\n",
    "# model y = sigmmoid(w^T*x + b)\n",
    "# input x\n",
    "x = tf.placeholder(tf.float32, shape=[None, M])\n",
    "# teacher\n",
    "t = tf.placeholder(tf.float32, shape=[None, K])\n",
    "# output y\n",
    "y = tf.nn.softmax(tf.matmul(x, w) + b) \n",
    "\n",
    "cross_entropy = tf.reduce_mean(- tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "# 確認\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t,1))\n",
    "\n",
    "# 初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 50\n",
    "n_batches = N\n",
    "\n",
    "for epoch in range(20):\n",
    "    X_, Y_ = shuffle(X, Y)\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict={\n",
    "                x: X_[start:end],\n",
    "                t: Y_[start:end]\n",
    "            })\n",
    "X_,Y_ = shuffle(X,Y)\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "        x: X_[0:10],\n",
    "        t: Y_[0:10]\n",
    "    })\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "        x: X_[0:10]\n",
    "    })\n",
    "\n",
    "print('classified:')\n",
    "print(classified)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "300/300 [==============================] - 0s - loss: 0.5266     \n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 0s - loss: 0.0506     \n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 0s - loss: 0.0421     \n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 0s - loss: 0.0382     \n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 0s - loss: 0.0363     \n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 0s - loss: 0.0335     \n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 0s - loss: 0.0319     \n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 0s - loss: 0.0305     \n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 0s - loss: 0.0291     \n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 0s - loss: 0.0283     \n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 0s - loss: 0.0272     \n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 0s - loss: 0.0261     \n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 0s - loss: 0.0252     \n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 0s - loss: 0.0247     \n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 0s - loss: 0.0238     \n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 0s - loss: 0.0232     \n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 0s - loss: 0.0225     \n",
      "Epoch 19/20\n",
      "300/300 [==============================] - 0s - loss: 0.0220     \n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 0s - loss: 0.0214     \n",
      "10/10 [==============================] - 0s\n",
      " 1/10 [==>...........................] - ETA: 0sclassified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  9.94871140e-01   5.12885069e-03   4.84665974e-10]\n",
      " [  1.41338666e-12   7.51294137e-04   9.99248683e-01]\n",
      " [  2.51148009e-11   5.90686745e-04   9.99409318e-01]\n",
      " [  1.00583715e-10   3.27876024e-03   9.96721208e-01]\n",
      " [  8.14356049e-09   3.23745236e-02   9.67625439e-01]\n",
      " [  3.31170362e-08   2.30615400e-02   9.76938426e-01]\n",
      " [  1.29235211e-09   2.94569205e-03   9.97054338e-01]\n",
      " [  1.39948986e-02   9.81299698e-01   4.70535131e-03]\n",
      " [  1.46598046e-11   7.61127099e-04   9.99238849e-01]\n",
      " [  1.06754384e-12   1.33569949e-04   9.99866366e-01]]\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=M, units = K))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1))\n",
    "\n",
    "minibatch_size = 50\n",
    "model.fit(X, Y, epochs=20, batch_size=minibatch_size)\n",
    "\n",
    "X_, Y_ = shuffle(X, Y)\n",
    "classes = model.predict_classes(X_[0:10], batch_size=minibatch_size)\n",
    "prob = model.predict_proba(X_[0:10], batch_size=1)\n",
    "\n",
    "print('classified:')\n",
    "print(np.argmax(model.predict(X_[0:10]), axis=1) == classes)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maeda\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEwUREfn6f4fioelrKW8KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231821aaf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.scatter(X1[:,0], X1[:,1], c='red')\n",
    "ax.scatter(X2[:,0], X2[:,1], c='blue')\n",
    "ax.scatter(X3[:,0], X3[:,1], c='green')\n",
    "\n",
    "\n",
    "w11 = sess.run(w)[0][0]\n",
    "w12 = sess.run(w)[0][1]\n",
    "b1 = sess.run(b)[0]\n",
    "linex = np.arange(-3, 5)\n",
    "liney = - linex * w11 / w12 - b1 * w11 / w12\n",
    "ax.plot(linex, liney, label='zero/one', color='red', linestyle='--')\n",
    "linex = np.arange(-3, 5)\n",
    "\n",
    "w21 = sess.run(w)[1][1]\n",
    "w22 = sess.run(w)[1][2]\n",
    "b2 = sess.run(b)[1]\n",
    "linex = np.arange(-3, 14)\n",
    "liney = - linex * w21 / w22 - b2 * w21 / w22\n",
    "ax.plot(linex, liney, label='one/two', color='red', linestyle='--')\n",
    "\n",
    "ax.set_title('plot')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
