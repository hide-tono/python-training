{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[ 0.22355038]\n",
      " [ 0.91425949]\n",
      " [ 0.91425949]\n",
      " [ 0.99747425]]\n",
      "w: [[ 3.61188436]\n",
      " [ 3.61188436]]\n",
      "b: [-1.24509501]\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow logistic regression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(0)  # 乱数シード\n",
    "\n",
    "# weight, bias\n",
    "w = tf.Variable(tf.zeros([2, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# model y = sigmmoid(w^T*x + b)\n",
    "# input x\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "# teacher\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "# output y\n",
    "y = tf.nn.sigmoid(tf.matmul(x, w) + b) \n",
    "\n",
    "cross_entropy = - tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "# 確認\n",
    "correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "\n",
    "# OR logic\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(200):\n",
    "    sess.run(train_step, feed_dict={\n",
    "            x: X,\n",
    "            t: Y\n",
    "        })\n",
    "\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })\n",
    "print(classified)\n",
    "\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })\n",
    "\n",
    "print(prob)\n",
    "\n",
    "# 学習したパラメータの確認\n",
    "print('w:', sess.run(w))\n",
    "print('b:', sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s - loss: 0.7670     \n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s - loss: 0.6901     \n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s - loss: 0.6319     \n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s - loss: 0.5884     \n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s - loss: 0.5546     \n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s - loss: 0.5287     \n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s - loss: 0.5073     \n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s - loss: 0.4901     \n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s - loss: 0.4750     \n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s - loss: 0.4624     \n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s - loss: 0.4520     \n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s - loss: 0.4423     \n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s - loss: 0.4338     \n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s - loss: 0.4258     \n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s - loss: 0.4188     \n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s - loss: 0.4121     \n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s - loss: 0.4059     \n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s - loss: 0.4000     \n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s - loss: 0.3944     \n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s - loss: 0.3889     \n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s - loss: 0.3835     \n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s - loss: 0.3786     \n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s - loss: 0.3737     \n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s - loss: 0.3691     \n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s - loss: 0.3645     \n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s - loss: 0.3602     \n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s - loss: 0.3560     \n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s - loss: 0.3518     \n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s - loss: 0.3479     \n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s - loss: 0.3439     \n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s - loss: 0.3400     \n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s - loss: 0.3362     \n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s - loss: 0.3325     \n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s - loss: 0.3290     \n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s - loss: 0.3254     \n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s - loss: 0.3219     \n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s - loss: 0.3185     \n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s - loss: 0.3152     \n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s - loss: 0.3119     \n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s - loss: 0.3087     \n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s - loss: 0.3056     \n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s - loss: 0.3025     \n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s - loss: 0.2995     \n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s - loss: 0.2965     \n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s - loss: 0.2936     \n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s - loss: 0.2907     \n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s - loss: 0.2879     \n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s - loss: 0.2851     \n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s - loss: 0.2824     \n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s - loss: 0.2797     \n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s - loss: 0.2771     \n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s - loss: 0.2745     \n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s - loss: 0.2720     \n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s - loss: 0.2695     \n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s - loss: 0.2670     \n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s - loss: 0.2646     \n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s - loss: 0.2622     \n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s - loss: 0.2599     \n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s - loss: 0.2576     \n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s - loss: 0.2553     \n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s - loss: 0.2531     \n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s - loss: 0.2509     \n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s - loss: 0.2487     \n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s - loss: 0.2466     \n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s - loss: 0.2446     \n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s - loss: 0.2425     \n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s - loss: 0.2404     \n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s - loss: 0.2385     \n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s - loss: 0.2365     \n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s - loss: 0.2346     \n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s - loss: 0.2327     \n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s - loss: 0.2308     \n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s - loss: 0.2290     \n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s - loss: 0.2271     \n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s - loss: 0.2254     \n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s - loss: 0.2236     \n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s - loss: 0.2218     \n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s - loss: 0.2201     \n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s - loss: 0.2184     \n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s - loss: 0.2168     \n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s - loss: 0.2151     \n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s - loss: 0.2135     \n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s - loss: 0.2119     \n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s - loss: 0.2103     \n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s - loss: 0.2088     \n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s - loss: 0.2072     \n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s - loss: 0.2057     \n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s - loss: 0.2042     \n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s - loss: 0.2028     \n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s - loss: 0.2013     \n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s - loss: 0.1999     \n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s - loss: 0.1985     \n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s - loss: 0.1970     \n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s - loss: 0.1957     \n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s - loss: 0.1943     \n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s - loss: 0.1930     \n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s - loss: 0.1916     \n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s - loss: 0.1903     \n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s - loss: 0.1891     \n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s - loss: 0.1878     \n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s - loss: 0.1853     \n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s - loss: 0.1840     \n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s - loss: 0.1828     \n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s - loss: 0.1816     \n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s - loss: 0.1804     \n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s - loss: 0.1793     \n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s - loss: 0.1781     \n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s - loss: 0.1770     \n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s - loss: 0.1758     \n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s - loss: 0.1747     \n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s - loss: 0.1736     \n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s - loss: 0.1725     \n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s - loss: 0.1715     \n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s - loss: 0.1704     \n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s - loss: 0.1693     \n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s - loss: 0.1683     \n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s - loss: 0.1673     \n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s - loss: 0.1663     \n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s - loss: 0.1653     \n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s - loss: 0.1643     \n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s - loss: 0.1633     \n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s - loss: 0.1623     \n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s - loss: 0.1614     \n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s - loss: 0.1604     \n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s - loss: 0.1595     \n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s - loss: 0.1585     \n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s - loss: 0.1576     \n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s - loss: 0.1567     \n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s - loss: 0.1558     \n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s - loss: 0.1549     \n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s - loss: 0.1540     \n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s - loss: 0.1532     \n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s - loss: 0.1523     \n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s - loss: 0.1515     \n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s - loss: 0.1506     \n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s - loss: 0.1498     \n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s - loss: 0.1490     \n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s - loss: 0.1481     \n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s - loss: 0.1473     \n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s - loss: 0.1465     \n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s - loss: 0.1457     \n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s - loss: 0.1450     \n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s - loss: 0.1442     \n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s - loss: 0.1434     \n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s - loss: 0.1427     \n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s - loss: 0.1419     \n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s - loss: 0.1411     \n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s - loss: 0.1404     \n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s - loss: 0.1397     \n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s - loss: 0.1390     \n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s - loss: 0.1382     \n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s - loss: 0.1375     \n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s - loss: 0.1368     \n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s - loss: 0.1361     \n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s - loss: 0.1354     \n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s - loss: 0.1347     \n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s - loss: 0.1341     \n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s - loss: 0.1334     \n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s - loss: 0.1321     \n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s - loss: 0.1314     \n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s - loss: 0.1308     \n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s - loss: 0.1301     \n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s - loss: 0.1295     \n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s - loss: 0.1282     \n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s - loss: 0.1276     \n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s - loss: 0.1252     \n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s - loss: 0.1246     \n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s - loss: 0.1240     \n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s - loss: 0.1234     \n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s - loss: 0.1229     \n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s - loss: 0.1223     \n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s - loss: 0.1217     \n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s - loss: 0.1212     \n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s - loss: 0.1206     \n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s - loss: 0.1201     \n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s - loss: 0.1195     \n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s - loss: 0.1190     \n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s - loss: 0.1184     \n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s - loss: 0.1179     \n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s - loss: 0.1174     \n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s - loss: 0.1169     \n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s - loss: 0.1163     \n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s - loss: 0.1158     \n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s - loss: 0.1153     \n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s - loss: 0.1148     \n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s - loss: 0.1143     \n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s - loss: 0.1138     \n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s - loss: 0.1133     \n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s - loss: 0.1128     \n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s - loss: 0.1123     \n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s - loss: 0.1119     \n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s - loss: 0.1114     \n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s - loss: 0.1109     \n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s - loss: 0.1104     \n",
      "1/4 [======>.......................] - ETA: 0sclassified\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "output probability:\n",
      "[[ 0.22441491]\n",
      " [ 0.91010392]\n",
      " [ 0.91771209]\n",
      " [ 0.9974438 ]]\n"
     ]
    }
   ],
   "source": [
    "# Keras logistic regression\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_dim=2, units=1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[1]])\n",
    "model.fit(X,Y,epochs=200,batch_size=1)\n",
    "classes = model.predict_classes(X, batch_size=1)\n",
    "prob = model.predict_proba(X, batch_size=1)\n",
    "\n",
    "print('classified')\n",
    "print(Y == classes)\n",
    "print()\n",
    "\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  3.63106600e-10   1.19366357e-03   9.98806357e-01]\n",
      " [  9.87981379e-01   1.20186592e-02   8.17272028e-09]\n",
      " [  9.57099557e-01   4.28982526e-02   2.11376823e-06]\n",
      " [  9.94710922e-01   5.28907729e-03   4.59038105e-08]\n",
      " [  9.98629808e-01   1.37017854e-03   3.81884357e-10]\n",
      " [  2.33484201e-08   1.17349634e-02   9.88265038e-01]\n",
      " [  9.64107990e-01   3.58918570e-02   7.17201303e-08]\n",
      " [  9.99247432e-01   7.52614113e-04   2.37509429e-10]\n",
      " [  1.05783418e-08   8.57559498e-03   9.91424322e-01]\n",
      " [  4.42274001e-07   6.35365099e-02   9.36462998e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Multi Logistic Regression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "M = 2 # dimension of input\n",
    "K = 3 # number of class\n",
    "n = 100 # number of data per class\n",
    "N = n * K # all data\n",
    "\n",
    "# sample data\n",
    "X1 = np.random.randn(n, M) + np.array([0, 10])\n",
    "X2 = np.random.randn(n, M) + np.array([5, 5])\n",
    "X3 = np.random.randn(n, M) + np.array([10, 0])\n",
    "Y1 = np.array([[1,0,0] for _ in range(n)])\n",
    "Y2 = np.array([[0,1,0] for _ in range(n)])\n",
    "Y3 = np.array([[0,0,1] for _ in range(n)])\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis=0)\n",
    "Y = np.concatenate((Y1, Y2, Y3), axis=0)\n",
    "\n",
    "tf.set_random_seed(0)  # 乱数シード\n",
    "\n",
    "# weight, bias\n",
    "w = tf.Variable(tf.zeros([M, K]))\n",
    "b = tf.Variable(tf.zeros([K]))\n",
    "\n",
    "# model y = sigmmoid(w^T*x + b)\n",
    "# input x\n",
    "x = tf.placeholder(tf.float32, shape=[None, M])\n",
    "# teacher\n",
    "t = tf.placeholder(tf.float32, shape=[None, K])\n",
    "# output y\n",
    "y = tf.nn.softmax(tf.matmul(x, w) + b) \n",
    "\n",
    "cross_entropy = tf.reduce_mean(- tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "# 確認\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t,1))\n",
    "\n",
    "# 初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 50\n",
    "n_batches = N\n",
    "\n",
    "for epoch in range(20):\n",
    "    X_, Y_ = shuffle(X, Y)\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict={\n",
    "                x: X_[start:end],\n",
    "                t: Y_[start:end]\n",
    "            })\n",
    "X_,Y_ = shuffle(X,Y)\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "        x: X_[0:10],\n",
    "        t: Y_[0:10]\n",
    "    })\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "        x: X_[0:10]\n",
    "    })\n",
    "\n",
    "print('classified:')\n",
    "print(classified)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "300/300 [==============================] - 0s - loss: 2.3630     \n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 0s - loss: 0.1653     \n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 0s - loss: 0.1039     \n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 0s - loss: 0.0891     \n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 0s - loss: 0.0788     \n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 0s - loss: 0.0706     \n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 0s - loss: 0.0644     \n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 0s - loss: 0.0585     \n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 0s - loss: 0.0547     \n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 0s - loss: 0.0511     \n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 0s - loss: 0.0479     \n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 0s - loss: 0.0458     \n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 0s - loss: 0.0433     \n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 0s - loss: 0.0413     \n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 0s - loss: 0.0395     \n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 0s - loss: 0.0377     \n",
      "Epoch 19/20\n",
      "300/300 [==============================] - 0s - loss: 0.0366     \n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 0s - loss: 0.0349     \n",
      "10/10 [==============================] - 0s\n",
      " 1/10 [==>...........................] - ETA: 0sclassified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  9.93194699e-01   6.80520665e-03   1.00135516e-07]\n",
      " [  3.92747035e-09   4.99230018e-03   9.95007634e-01]\n",
      " [  1.16779586e-09   1.74373889e-03   9.98256266e-01]\n",
      " [  9.99421597e-01   5.78483392e-04   1.39274330e-11]\n",
      " [  5.31881629e-03   9.87796605e-01   6.88448129e-03]\n",
      " [  4.78517124e-03   9.47108030e-01   4.81066816e-02]\n",
      " [  9.59785759e-01   4.02142890e-02   1.85667375e-08]\n",
      " [  1.13518324e-08   6.12714747e-03   9.93872881e-01]\n",
      " [  4.17608442e-03   9.83705282e-01   1.21185929e-02]\n",
      " [  3.04051917e-09   5.54352254e-03   9.94456410e-01]]\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=M, units = K))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1))\n",
    "\n",
    "minibatch_size = 50\n",
    "model.fit(X, Y, epochs=20, batch_size=minibatch_size)\n",
    "\n",
    "X_, Y_ = shuffle(X, Y)\n",
    "classes = model.predict_classes(X_[0:10], batch_size=minibatch_size)\n",
    "prob = model.predict_proba(X_[0:10], batch_size=1)\n",
    "\n",
    "print('classified:')\n",
    "print(np.argmax(model.predict(X_[0:10]), axis=1) == classes)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maeda\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2115c3e4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.scatter(X1[:,0], X1[:,1], c='red')\n",
    "ax.scatter(X2[:,0], X2[:,1], c='blue')\n",
    "ax.scatter(X3[:,0], X3[:,1], c='green')\n",
    "\n",
    "ax.set_title('plot')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.08142 0.287688 -0.0396391\n",
      "Tensor(\"Softmax_3:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w11 = sess.run(w)[0][0]\n",
    "w12 = sess.run(w)[0][1]\n",
    "b1 = sess.run(b)[0]\n",
    "\n",
    "print(w11, w12, b1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
