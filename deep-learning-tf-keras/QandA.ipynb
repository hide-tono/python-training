{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P82 式3.17 -(t-y)は間違い？\n",
    "間違い。t-yが正解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法について\n",
    "ニューラルネットワークはパラメータを最適化したいというモチベーションしかない。\n",
    "誤差を最小化したい。そのために出てきたのが誤差逆伝播法。\n",
    "\n",
    "誤差関数\\\\( E_{(w,b)} \\\\)について\n",
    "\n",
    "\\\\( \\frac{\\partial E}{\\partial W} = 0 \\\\)にしたい。\n",
    "\n",
    "複雑なモデルの最適パラメータを求めるときには解析的には求められない。\n",
    "-> 勾配降下法を用いる\n",
    "\n",
    "P127\n",
    "\n",
    "基本は重みx入力\n",
    "\n",
    "$$ y = f(Wx + b) $$\n",
    "\n",
    "式3.96も同じ\n",
    "\n",
    "$$ \\delta_j = f'(p_j)\\sum_{k=1}^K v_{jk}\\delta_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数のおすすめ\n",
    "ReLUかLReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習率の設定\n",
    "Adam(計算量多め)かRMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "\n",
    "### DeepLearning発展の歴史\n",
    "\n",
    "* Deep Belief Nets(DBN)\n",
    "* Stacked denoising Autoencoders(SdA)\n",
    "\n",
    "上記２つは事前学習という教師なし学習のステップが含まれていた。\n",
    "\n",
    "* DBNはRBM(制限ボルツマンマシン）\n",
    "* SdAはdA( denoising Autoencoders)\n",
    "\n",
    "#### 手法\n",
    "\n",
    "入力(Visible) <-> 隠れ層 で 教師なし学習をする。(入力と出力が一致するように学習する。)\n",
    "\n",
    "隠れ層 -> 中間層\n",
    "\n",
    "中間層 <-> 隠れ層 でまた学習\n",
    "\n",
    "隠れ層 -> ロジスティック回帰\n",
    "\n",
    "最後に全体でFine Tuningする。\n",
    "\n",
    "上記の通り計算量が多い。\n",
    "\n",
    "### AutoEncoderの復活\n",
    "\n",
    "画像生成の分野で使われている。\n",
    "\n",
    "Variational Auto Encoders(VAE)など。\n",
    "\n",
    "#### 余談\n",
    "\n",
    "DLでは低次元レイヤーであるほど点や線などの特徴を抽出し、高次元になるほど輪郭などの情報を抽出する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章\n",
    "## sin波のRNN学習に必要なデータ\n",
    "全パターン(sin波の周期)を含むように予測する\n",
    "\n",
    "## なぜ重みは固定なのに全時系列を表現できるのか？\n",
    "\n",
    "教師データの中に共通のパターンが存在する。\n",
    "\n",
    "重みWはそのすべてのパターンを表現できるように調整されるだけ。\n",
    "\n",
    "__あらゆる関数はマクローリン展開により多項式で近似できる。__\n",
    "\n",
    "### 機械学習はそもそも何をしているか\n",
    "\n",
    "$$ y = f(x) $$\n",
    "\n",
    "入力に何らかの関数を当てはめると出力yがでる。その関数を求める。\n",
    "\n",
    "その神関数fを合成関数\n",
    "\n",
    "$$ f = g \\circ h \\circ i \\circ j \\circ k... $$\n",
    "\n",
    "で求める。それぞれの関数がニューラルネットワークの各層にあたる。\n",
    "\n",
    "合成関数をやりすぎるのがオーバーフィッティング。\n",
    "\n",
    "\n",
    "\n",
    "sin波の場合隠れ層1層なので、P.212の式(5.4)(5.5)のU,W,Tの３つのパラメータで近似ができるようになるということ。\n",
    "\n",
    "それ以上に複雑だと隠れ層を増やしたりLSTM(パラメータ15個)やGRU(パラメータ数8個)が必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CECで誤差をとどまらせるとは？\n",
    "\n",
    "CECの値を次の時系列でも用いるということ。\n",
    "\n",
    "式(5.30)や式(5.31)は1でなくても消えなければ良い。しかし、2とかだと勾配爆発する可能性があるので1にする。\n",
    "\n",
    "機械学習は物事を理想的な状態で記述することが多い。\n",
    "\n",
    "4.5章の重みの正則化についても同様。世の中が理想的な状態であったら、データが正規分布に従っていたら、といった仮定が多い。\n",
    "\n",
    "### 時間依存性のある信号・ない信号とは？\n",
    "\n",
    "判断式は式(5.36)。これが入力値を時間依存性ありなし判断を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### 忘却ゲート\n",
    "\n",
    "基本的なモチベーション：短期の周期と長期の周期が混ざったデータを判別したい。\n",
    "\n",
    "    wwwwWWWWWW\n",
    "    p波 s波\n",
    "\n",
    "となったとき、忘却ゲートがないとp波をCECが忘れられず、s波以降の予測に悪影響が出る\n",
    "\n",
    "\n",
    "### 覗き穴結合\n",
    "\n",
    "CECの状態がどのゲートからもわからないので、CEC自体の状態がわかればもっと良いのではないか？\n",
    "\n",
    "でもkerasでもchainerでも実装されておらず、あまり効果はない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Problem\n",
    "\n",
    "Adding ProblemはLSTMが最初に発表されたときに使われた。\n",
    "\n",
    "### P.245で誤差関数の値が0.1767になるのは？\n",
    "\n",
    "書いてあるとおり。\n",
    "\n",
    "### 誤差関数の選択肢は？\n",
    "\n",
    "微分\\\\( \\frac{\\partial E}{\\partial W} \\\\)が計算できればよい。計算しやすいものが選ばれる。\n",
    "\n",
    "機械学習は基本分類問題と回帰問題の2つに分けられる。\n",
    "\n",
    "e.g.\n",
    "* 株価が上がるか下がるかは分類。 Logistic Regression使うのでCross Entropy誤差関数を使うのが普通。\n",
    "* 株価がいくらになるかが回帰。 二条誤差関数を使うのが普通。\n",
    "\n",
    "## GRUがLSTMに勝る？\n",
    "\n",
    "GRUはパラメータ数がLSTMより少ないのでGRUのほうが早いのは確実だが、精度はケース・バイ・ケース。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第６章\n",
    "## attentionとは\n",
    "時間に重みをつける。どの過去の値が重要なのかというのを決める。FXでは遅行スパンとかが該当する？\n",
    "\n",
    "## BiRNN\n",
    "\n",
    "### それぞれ独立のNNで学習されたときとは異なるのか？\n",
    "\n",
    "異なる。式(6.3)で過去と未来の出力を合わせて出力するので、誤差が異なるようになる。\n",
    "\n",
    "__式(6.2)の(t-1)は(t+1)の誤りである。__\n",
    "\n",
    "## RNN Encoder-Decoder Sequence to Sequence\n",
    "\n",
    "### なぜ2つのRNNによるEncoder-Decoder構造にしなければならないのか？\n",
    "\n",
    "Encoder-Decoderだと逐次的に予測できるので。\n",
    "\n",
    "### 足し算問題のパラメータはどうなる？\n",
    "自分で計算しよう。\n",
    "\n",
    "attention と RNN Encoder-Decoderは組み合わせ可能。\n",
    "\n",
    "attentionを入れてもそんなに前のことを見ていないというデータもあり、ケース・バイ・ケースだが基本入れておいたほうが良い。\n",
    "\n",
    "### Memory Networksは複雑になっている？\n",
    "\n",
    "外部記憶は構造上外部にあるように見えているが、実際はニューラルネットワーク内に組み込まれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全般\n",
    "## ChainerやPyTorch\n",
    "TensorFlowやKerasのほうがグローバルの人気やバックアップが強いので。\n",
    "\n",
    "数式レベルでいじりたいならTensorFlowを使おう。\n",
    "\n",
    "## 時系列解析のネットワーク層数\n",
    "\n",
    "バーニーおじさんのルール…パラメータ数の10倍のデータ数があるといい\n",
    "\n",
    "逆算してパラメータ、層数を決めよう。\n",
    "\n",
    "## 新しいNNモデルを理解するコツ\n",
    "\n",
    "基本\n",
    "* \\\\( y=f(x) \\\\)のfをどう頑張るか\n",
    "* どう誤差を0に近づけるのか→どう微分をしているのか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
