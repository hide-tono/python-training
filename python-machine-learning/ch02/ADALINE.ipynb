{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE\n",
    "パーセプトロンとの違いは、重みの更新方法\n",
    "* パーセプトロン：単位ステップ関数\n",
    "* ADALINE：線形活性化関数 \\\\(\\phi(z)\\\\)\n",
    "\n",
    "目的関数(Objective function) ... 学習過程で最適化される関数。多くの場合は ** コスト関数 ** (cost function)\n",
    "\n",
    "## コスト関数\n",
    "誤差平方和(Sum of Squared Error：SSE)\n",
    "$$ J(w) = \\frac{1}{2}\\sum_i(y^{(i)}-\\phi(z^{(i)}))^2 $$\n",
    "\n",
    "利点\n",
    "* 微分可能である\n",
    "* 凸関数であるため勾配降下法(gradient descent)を用いてコスト関数を最小化する重みを見つけ出すことができる。\n",
    "\n",
    "### 勾配降下法を使った重み更新\n",
    "コスト関数\\\\( J(w) \\\\)の勾配\\\\( \\nabla J(w) \\\\)に沿って1ステップ進む\n",
    "$$ w := w + \\Delta w $$\n",
    "重みの変化である\\\\( \\Delta w \\\\)は、負の勾配に学習率\\\\( \\eta \\\\)を掛けたもの\n",
    "$$ \\Delta w = -\\eta\\nabla J(w) $$\n",
    "\n",
    "### 勾配計算(偏微分係数)\n",
    "$$ \\begin{align} \\frac{\\delta J}{\\delta w_j} = \\frac{\\delta}{\\delta w_j}\\frac{1}{2}\\sum_i(y^{(i)}-\\phi(z^{(i)}))^2 \\\\\n",
    "= \\frac{1}{2}\\frac{\\delta}{\\delta w_j}\\sum_i(y^{(i)}-\\phi(z^{(i)}))^2 \\\\\n",
    "= \\frac{1}{2}\\sum_i2(y^{(i)}-\\phi(z^{(i)}))\\frac{\\delta}{\\delta w_j}(y^{(i)}-\\phi(z^{(i)})) \\\\\n",
    "= \\sum_i(y^{(i)}-\\phi(z^{(i)}))\\frac{\\delta}{\\delta w_j}\\Bigl( y^{(i)}-\\sum_k(w_kx_k^{(i)})\\Bigr) \\\\\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
